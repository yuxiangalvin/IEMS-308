{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# Import NLTK module\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "\n",
    "import re\n",
    "import string \n",
    "\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_labels = np.array(pd.read_excel('./labels/companies.xlsx', names = ['company name'], dtype=str)['company name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_name_database = pd.read_csv('company_name_database.csv')\n",
    "# company_name_database = company_name_database['Company Name']\n",
    "# company_name_database= company_name_database.values\n",
    "# company_name_database_first_word = [i.split()[0] for i in company_name_database]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cleanco import typesources, countrysources, matches\n",
    "from cleanco import cleanco\n",
    "# classification_sources = typesources()\n",
    "# # classification_sources = countrysources()\n",
    "# matches(\"Apple\", classification_sources)\n",
    "x = cleanco('')\n",
    "x.country()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_company_labels = set()\n",
    "for company in company_labels:\n",
    "    clean_company = cleanco(company).clean_name()\n",
    "    clean_company = clean_company.replace(',', '')\n",
    "    clean_company = clean_company.replace('.', '')\n",
    "    clean_company = clean_company.replace(' ', '')\n",
    "    clean_company_labels.add(clean_company.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(company_name_database_first_word)):\n",
    "#     if company_name_database_first_word[i][-1] == ',' or company_name_database_first_word[i][-1] == '.':\n",
    "#         company_name_database_first_word[i] = company_name_database_first_word[i][:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for company in company_labels['company name']:\n",
    "#     company = company.upper()\n",
    "#     if company not in company_name_database:\n",
    "#         if company.split()[0] not in company_name_database_first_word:\n",
    "#             print(company)\n",
    "#             count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_match(str1,str2):\n",
    "#     str1=str1.replace('.','')\n",
    "#     str2=str2.replace('.','')\n",
    "#     str1=str1.replace(',','')\n",
    "#     str2=str2.replace(',','')\n",
    "    \n",
    "#     lenth = len(str1.split())\n",
    "\n",
    "#     str2 = ' '.join(str2.split()[:lenth])\n",
    "#     return str1.upper() == str2.upper()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_name(name):\n",
    "#     clean_company = cleanco(name).clean_name()\n",
    "#     clean_company = clean_company.replace(',', '')\n",
    "#     clean_company = clean_company.replace('.', '')\n",
    "#     clean_company = clean_company.replace(' ', '')\n",
    "#     return clean_company.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_the(entity):\n",
    "    entity = entity.replace('the ', '')\n",
    "    entity = entity.replace('The ', '')\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = []\n",
    "entity_start_sentence_index = []\n",
    "entity_is_ORG = []\n",
    "entity_text_original_length = []\n",
    "entity_text_all_upper = []\n",
    "entity_text_all_lower = []\n",
    "entity_text_istitle = []\n",
    "entity_text_isdigit = []\n",
    "entity_text_num_word = []\n",
    "entity_text_has_company_suffix = []\n",
    "word_around_has_company = []\n",
    "word_around_has_possessive = []\n",
    "word_around_list = []\n",
    "company_true_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/730\n",
      "20/730\n",
      "30/730\n",
      "40/730\n",
      "50/730\n",
      "60/730\n",
      "70/730\n",
      "80/730\n",
      "90/730\n",
      "100/730\n",
      "110/730\n",
      "120/730\n",
      "130/730\n",
      "140/730\n",
      "150/730\n",
      "160/730\n",
      "170/730\n",
      "180/730\n",
      "190/730\n",
      "200/730\n",
      "210/730\n",
      "220/730\n",
      "230/730\n",
      "240/730\n",
      "250/730\n",
      "260/730\n",
      "270/730\n",
      "280/730\n",
      "290/730\n",
      "300/730\n",
      "310/730\n",
      "320/730\n",
      "330/730\n",
      "340/730\n",
      "350/730\n",
      "360/730\n",
      "370/730\n",
      "380/730\n",
      "390/730\n",
      "400/730\n",
      "410/730\n",
      "420/730\n",
      "430/730\n",
      "440/730\n",
      "450/730\n",
      "460/730\n",
      "470/730\n",
      "480/730\n",
      "490/730\n",
      "500/730\n",
      "510/730\n",
      "520/730\n",
      "530/730\n",
      "540/730\n",
      "550/730\n",
      "560/730\n",
      "570/730\n",
      "580/730\n",
      "590/730\n",
      "600/730\n",
      "610/730\n",
      "620/730\n",
      "630/730\n",
      "640/730\n",
      "650/730\n",
      "660/730\n",
      "670/730\n",
      "680/730\n",
      "690/730\n",
      "700/730\n",
      "710/730\n",
      "720/730\n",
      "730/730\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "count = 0\n",
    "directory_list = [f'./BI-articles/2013',f'./BI-articles/2014']\n",
    "for directory in directory_list:\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".txt\"):               \n",
    "\n",
    "            with open(directory +'/'+ filename,'r', errors='replace') as f:\n",
    "                lines = f.readlines()\n",
    "            out = [(\" \" if line.startswith(\" \") else \"\\n\") + line.strip() for line in lines]\n",
    "            res = ''.join(out).split('\\n')[1:]\n",
    "            article_str = ' '.join(res)\n",
    "            article_str = article_str.replace(u'\\xa0', u' ')\n",
    "\n",
    "            nlp_result = nlp(article_str)\n",
    "            ents = nlp_result.ents\n",
    "            \n",
    "            count += 1\n",
    "                        \n",
    "            if count % 10 == 0:\n",
    "                print(str(count) + '/' + '730')\n",
    "                \n",
    "            \n",
    "\n",
    "            for ent in ents:\n",
    "\n",
    "\n",
    "                entity_start_sentence_index.append(ent.start_char - ent.sent.start_char)\n",
    "                \n",
    "                if ent.label_ == 'ORG':\n",
    "                    entity_is_ORG.append(True)\n",
    "                else:\n",
    "                    entity_is_ORG.append(False)\n",
    "                #features based on just the entity word\n",
    "                entity_text = remove_the(entity_text)\n",
    "                all_entities.append(entity_text)\n",
    "                entity_text_original_length.append(len(entity_text))        \n",
    "                entity_text_all_upper.append(entity_text.isupper())\n",
    "                entity_text_all_lower.append(entity_text.islower())\n",
    "                entity_text_istitle.append(entity_text.istitle())\n",
    "                entity_text_isdigit.append(entity_text.isdigit())\n",
    "                entity_text_num_word.append(len(entity_text.split(' ')))\n",
    "                entity_text_has_company_suffix.append(cleanco(entity_text).clean_name()!=entity_text)\n",
    "                \n",
    "                #features based on word around\n",
    "                word_around = article_str[(ent.start_char-50) : (ent.end_char+50)]\n",
    "                word_around_list.append(word_around)\n",
    "                word_around_has_company.append('company' in word_around.lower())\n",
    "                word_around_has_possessive.append('\\'s' in word_around)\n",
    "                \n",
    "                \n",
    "\n",
    "                #check whether it is in true company_labels\n",
    "                if entity_text  in company_labels:\n",
    "                    company_true_labels.append(True)\n",
    "                else:\n",
    "                    company_true_labels.append(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# result_dict = {}\n",
    "# count = 0\n",
    "# directory_list = [f'./BI-articles/2013',f'./BI-articles/2014']\n",
    "# for directory in directory_list:\n",
    "#     for file in os.listdir(directory):\n",
    "#         filename = os.fsdecode(file)\n",
    "#         if filename.endswith(\".txt\"):               \n",
    "\n",
    "#             with open(directory +'/'+ filename,'r', errors='replace') as f:\n",
    "#                 lines = f.readlines()\n",
    "#             out = [(\" \" if line.startswith(\" \") else \"\\n\") + line.strip() for line in lines]\n",
    "#             res = ''.join(out).split('\\n')[1:]\n",
    "#             article_str = ' '.join(res)\n",
    "#             article_str = article_str.replace(u'\\xa0', u' ')\n",
    "\n",
    "#             nlp_result = nlp(article_str)\n",
    "#             ents = nlp_result.ents\n",
    "            \n",
    "#             count += 1\n",
    "                        \n",
    "#             if count % 10 == 0:\n",
    "#                 print(str(count) + '/' + '730')\n",
    "                \n",
    "            \n",
    "\n",
    "#             for ent in ents:\n",
    "#                 if ent.label_ == 'ORG':\n",
    "#                     all_orgs.append(ent.text)\n",
    "                    \n",
    "#                     org_start_sentence_index.append(ent.start_char - ent.sent.start_char)\n",
    "#                     org_end_sentence_index.append(ent.end_char - ent.sent.start_char)        \n",
    "#                     sentence = article_str[ent.sent.start_char : ent.sent.end_char]\n",
    "#                     all_sentences.append(sentence)\n",
    "                    \n",
    "#                     org_name = ent.text\n",
    "#                     org_name_original_lenth.append(len(org_name))\n",
    "#                     num_capital_letter.append(sum(map(str.isupper, org_name)))\n",
    "#                     num_word.append(len(org_name.split(' ')))\n",
    "                    \n",
    "#                     clean_org_name = clean_name(org_name)\n",
    "#                     if clean_org_name in clean_company_labels:\n",
    "#                         company_true_labels.append(True)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = np.array(all_entities)\n",
    "entity_start_sentence_index = np.array(entity_start_sentence_index)\n",
    "entity_is_ORG = np.array(entity_is_ORG)\n",
    "entity_text_original_length = np.array(entity_text_original_length)\n",
    "entity_text_all_upper = np.array(entity_text_all_upper)\n",
    "entity_text_all_lower = np.array(entity_text_all_lower)\n",
    "entity_text_istitle = np.array(entity_text_istitle)\n",
    "entity_text_isdigit = np.array(entity_text_isdigit)\n",
    "entity_text_num_word = np.array(entity_text_num_word)\n",
    "entity_text_has_company_suffix = np.array(entity_text_has_company_suffix)\n",
    "word_around_has_company = np.array(word_around_has_company)\n",
    "word_around_has_possessive = np.array(word_around_has_possessive)\n",
    "# word_around_list = np.array(word_around_list)\n",
    "company_true_labels = np.array(company_true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_end_with_str(entity, end):\n",
    "    if entity[(-len(end)):] == end:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_end_with_Inc = []\n",
    "# entity_end_with_Ltd = []\n",
    "# entity_end_with_LLC = []\n",
    "# entity_end_with_Co = []\n",
    "# entity_end_with_Corp = []\n",
    "# entity_end_with_Group = []\n",
    "# entity_end_with_Corporation = []\n",
    "# entity_end_with_Company = []\n",
    "\n",
    "entity_end_with_company_name_suffix= []\n",
    "for entity in all_entities:\n",
    "    if entity[-1] == '.':\n",
    "        entity = entity[:-1]\n",
    "    company_end_list = ['Inc', 'Ltd', 'LLC', 'Co', 'Corp', 'Group', 'Corporation', 'Company']\n",
    "    find_end = False\n",
    "    for company_end in company_end_list:\n",
    "        if check_end_with_str(entity, company_end):        \n",
    "            find_end = True\n",
    "            break\n",
    "    \n",
    "    entity_end_with_company_name_suffix.append(find_end)\n",
    "            \n",
    "#     entity_end_with_Inc.append(check_end_with_str(entity, 'Inc'))\n",
    "#     entity_end_with_Ltd.append(check_end_with_str(entity, 'Ltd'))\n",
    "#     entity_end_with_LLC.append(check_end_with_str(entity, 'LLC'))\n",
    "#     entity_end_with_Co.append(check_end_with_str(entity, 'Co'))\n",
    "#     entity_end_with_Corp.append(check_end_with_str(entity, 'Corp'))\n",
    "#     entity_end_with_Group.append(check_end_with_str(entity, 'Group'))\n",
    "#     entity_end_with_Corporation.append(check_end_with_str(entity, 'Corporation'))\n",
    "#     entity_end_with_Company.append(check_end_with_str(entity, 'Company'))\n",
    "        \n",
    "entity_end_with_company_name_suffix = np.array(entity_end_with_company_name_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(all_entities)):\n",
    "#     if all_entities[i][-2:] == '\\'s':\n",
    "#         all_entities[i] = all_entities[i][:-2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_text_has_number = []\n",
    "for entity in all_entities:\n",
    "    entity_text_has_number.append(bool(re.search(r'\\d', entity)))\n",
    "entity_text_has_number = np.array(entity_text_has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_text_has_not_letter_number= []\n",
    "for entity in all_entities:\n",
    "    entity_text_has_not_letter_number.append(bool(re.search(r'[^a-zA-Z0-9.\\s\\']', entity)))\n",
    "entity_text_has_number = np.array(entity_text_has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(data = np.array([company_true_labels,\n",
    "                                           entity_text_all_upper, entity_text_all_lower,\n",
    "                                           entity_text_istitle, entity_text_has_number,\n",
    "                                           entity_text_has_not_letter_number,\n",
    "                                           entity_is_ORG,\n",
    "                                           word_around_has_company, word_around_has_possessive,\n",
    "                                           entity_end_with_company_name_suffix]).T, \n",
    "                          \n",
    "                          columns = ['company_true_labels',\n",
    "                                     'entity_text_all_upper', 'entity_text_all_lower',\n",
    "                                     'entity_text_istitle', 'entity_text_has_number', \n",
    "                                     'entity_text_has_not_letter_number',\n",
    "                                     'entity_is_ORG',\n",
    "                                     'word_around_has_company', 'word_around_has_possessive',\n",
    "                                    'entity_end_with_company_name_suffix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df.to_csv('company_summary_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df_pos_samples = summary_df[summary_df['company_true_labels'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(summary_df_pos_samples['entity_text_all_lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(summary_df_pos_samples['entity_text_has_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1341857"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = summary_df[(summary_df['entity_text_all_lower'] == False) & (summary_df['entity_text_has_number'] == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830738"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = summary_df.drop(columns=['entity_text_all_lower', 'entity_text_has_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_entities = []\n",
    "for i in range(len(all_entities)):\n",
    "    if (entity_text_all_lower[i] == False) & (entity_text_has_number[i] == False):\n",
    "        remain_entities.append(all_entities[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(remain_entities) == len(summary_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df_pos_samples = summary_df[summary_df['company_true_labels'] == True]\n",
    "summary_df_neg_samples = summary_df[summary_df['company_true_labels'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_n = summary_df_neg_samples.shape[0]\n",
    "pos_sample_n = neg_sample_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df_oversample_pos_samples = summary_df_pos_samples.sample(n=pos_sample_n, random_state=10, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data = pd.concat([summary_df_oversample_pos_samples, summary_df_neg_samples])\n",
    "used_x = used_data.iloc[:,1:]\n",
    "used_y = used_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(used_x,\n",
    "                                                    used_y, \n",
    "                                                    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "del summary_df_neg_samples\n",
    "del summary_df_pos_samples\n",
    "del summary_df_oversample_pos_samples\n",
    "del used_data\n",
    "del used_x\n",
    "del used_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.597903\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>company_true_labels</td> <th>  No. Observations:  </th>   <td>1186584</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>Logit</td>        <th>  Df Residuals:      </th>   <td>1186577</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>         <th>  Df Model:          </th>   <td>     6</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 02 Mar 2021</td>   <th>  Pseudo R-squ.:     </th>   <td>0.1374</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:56:00</td>       <th>  Log-Likelihood:    </th> <td>-7.0946e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>              <td>True</td>         <th>  LL-Null:           </th> <td>-8.2248e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>      <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                   <td></td>                      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>entity_text_all_upper</th>               <td>   -1.0681</td> <td>    0.007</td> <td> -161.496</td> <td> 0.000</td> <td>   -1.081</td> <td>   -1.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>entity_text_istitle</th>                 <td>   -0.6499</td> <td>    0.003</td> <td> -217.499</td> <td> 0.000</td> <td>   -0.656</td> <td>   -0.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>entity_text_has_not_letter_number</th>   <td>   -2.4232</td> <td>    0.014</td> <td> -167.653</td> <td> 0.000</td> <td>   -2.451</td> <td>   -2.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>entity_is_ORG</th>                       <td>    1.6251</td> <td>    0.004</td> <td>  408.294</td> <td> 0.000</td> <td>    1.617</td> <td>    1.633</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_around_has_company</th>             <td>    0.5279</td> <td>    0.012</td> <td>   44.343</td> <td> 0.000</td> <td>    0.505</td> <td>    0.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_around_has_possessive</th>          <td>   -0.0798</td> <td>    0.005</td> <td>  -16.791</td> <td> 0.000</td> <td>   -0.089</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>entity_end_with_company_name_suffix</th> <td>    0.6348</td> <td>    0.019</td> <td>   32.649</td> <td> 0.000</td> <td>    0.597</td> <td>    0.673</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            Logit Regression Results                           \n",
       "===============================================================================\n",
       "Dep. Variable:     company_true_labels   No. Observations:              1186584\n",
       "Model:                           Logit   Df Residuals:                  1186577\n",
       "Method:                            MLE   Df Model:                            6\n",
       "Date:                 Tue, 02 Mar 2021   Pseudo R-squ.:                  0.1374\n",
       "Time:                         14:56:00   Log-Likelihood:            -7.0946e+05\n",
       "converged:                        True   LL-Null:                   -8.2248e+05\n",
       "Covariance Type:             nonrobust   LLR p-value:                     0.000\n",
       "=======================================================================================================\n",
       "                                          coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------\n",
       "entity_text_all_upper                  -1.0681      0.007   -161.496      0.000      -1.081      -1.055\n",
       "entity_text_istitle                    -0.6499      0.003   -217.499      0.000      -0.656      -0.644\n",
       "entity_text_has_not_letter_number      -2.4232      0.014   -167.653      0.000      -2.451      -2.395\n",
       "entity_is_ORG                           1.6251      0.004    408.294      0.000       1.617       1.633\n",
       "word_around_has_company                 0.5279      0.012     44.343      0.000       0.505       0.551\n",
       "word_around_has_possessive             -0.0798      0.005    -16.791      0.000      -0.089      -0.070\n",
       "entity_end_with_company_name_suffix     0.6348      0.019     32.649      0.000       0.597       0.673\n",
       "=======================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_logreg = sm.Logit(y_train, X_train).fit()\n",
    "sm_logreg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = list(map(round, sm_logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106092,  35602],\n",
       "       [ 42274, 112678]], dtype=int64)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7431801392992825"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=None)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = summary_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y = summary_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_label(pred_value):\n",
    "    if pred_value > 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all_y = np.array(list(map(predict_to_label, sm_logreg.predict(all_x)))) \n",
    "pred_all_y = (pred_all_y==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_company_list = []\n",
    "for i in range(len(pred_all_y)):\n",
    "    if pred_all_y[i]:\n",
    "        extract_company_list.append(remain_entities[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_entities = np.array(remain_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commidea Ltd\n",
      "Kroger Co\n",
      "Napster\n",
      "Staples Inc\n",
      "Tommy Hilfiger\n",
      "Murray Kessler\n",
      "Quicken Loans\n",
      "Marin Software\n",
      "Wonga\n",
      "Quicksilver\n",
      "Build America Mutual\n",
      "Town Residential\n",
      "Plated\n",
      "Eskom\n",
      "Canali\n",
      "Easyjet\n",
      "Eclipse\n",
      "Fendi\n",
      "Gancia\n",
      "Garland\n",
      "Github\n",
      "Gucci\n",
      "Ipanema\n",
      "Jiaflix\n",
      "Kowalski\n",
      "Scribd\n",
      "Sephora\n",
      "Yuebao\n",
      "Zara\n",
      "Emile Garcin\n",
      "Hewlett-Packard\n",
      "Kimco\n",
      "Snapfish\n",
      "Wal-Mart\n",
      "Braintree\n",
      "Alcatel-Lucent\n",
      "Nutronics Labs\n",
      "Waze\n",
      "Mercedes-Benz\n",
      "Hewlett-Packard Co\n",
      "E-Trade\n",
      "Alstom\n",
      "Spacex\n",
      "Dion Weisler\n",
      "Chimerix\n",
      "Luxottica\n",
      "Cenqua\n",
      "Siemans\n",
      "Walk Score\n",
      "Woodbridge\n",
      "Aldi Sud\n",
      "Ferrero\n",
      "T-Mobile\n",
      "AngelHack\n",
      "Tommy Hilfiger\n",
      "Laurent Potdevin\n",
      "Wal-Mart\n",
      "K. Hovnanian\n",
      "Exelon Corporation\n",
      "Nutronics Labs\n",
      "Wal-mart\n",
      "IDA Ireland\n",
      "Snapfish\n",
      "Alcatel-Lucent\n",
      "Alice Corp\n",
      "Ciena Corp\n",
      "Denso Corp\n",
      "Pharmacia Corp\n",
      "Taisei Corp\n",
      "Aeropostale Inc\n",
      "Agrium Inc\n",
      "Alice Corp\n",
      "Ann Inc\n",
      "Assa Co Ltd\n",
      "Astellas Pharma Inc\n",
      "Beyond Inc\n",
      "Biogen Idec Inc\n",
      "Breaux Lott Leadership Group\n",
      "Burberry Inc\n",
      "Canon Inc\n",
      "Ciena Corp\n",
      "Convertro Inc\n",
      "Cummins Inc\n",
      "Denso Corp\n",
      "Dongxiang\n",
      "Tommy Hilfiger\n",
      "Gristedes\n",
      "Gucci\n",
      "Indochino\n",
      "Luvo\n",
      "Magic Leap Inc\n",
      "Manitowoc Co Inc\n",
      "Marilyn Monrobot\n",
      "Marquis Jet\n",
      "Newport Beach\n",
      "Pantech Co Ltd\n",
      "Payward Inc\n",
      "Pharmacia Corp\n",
      "Rebel Desk\n",
      "Roadhouse Inc\n",
      "Safran\n",
      "South Bay Apparel Inc\n",
      "Staples Inc\n",
      "Steve Blumenthal\n",
      "Taisei Corp\n",
      "Tommy Hilfiger\n",
      "World Cup Group\n",
      "Wyeth\n",
      "Yves Saint Laurent\n",
      "Krispy Kreme\n",
      "Bank of Tokyo-Mitsubishi UFJ\n",
      "Clorox Co\n",
      "Covington\n",
      "T-Mobile\n",
      "Wal-Mart\n",
      "Graham Partners\n",
      "Quicken Loans\n",
      "CoinJar\n",
      "Alcatel-Lucent\n",
      "Nutronics Labs\n",
      "Nutronics Labs\n",
      "Wal-mart\n",
      "IDA Ireland\n",
      "Snapfish\n",
      "Alcatel-Lucent\n"
     ]
    }
   ],
   "source": [
    "found_count = 0\n",
    "for company_label in company_labels:\n",
    "    found = False\n",
    "    for company_found in extract_company_list:\n",
    "        if company_label in company_found:\n",
    "            found = True\n",
    "            break\n",
    "    if found:\n",
    "        found_count += 1\n",
    "    else:\n",
    "        if company_label in remain_entities:\n",
    "            print(company_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found proportion: 0.8474452554744526\n"
     ]
    }
   ],
   "source": [
    "print('found proportion:', found_count/len(company_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.unique(extract_company_list), name='Company names').to_excel('extracted companies.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Supreme Court', 'Wyckoff', 'Sweetwater Travel', 'Job Corps',\n",
       "       'Raider', 'Polite Provisions', 'Hiro', 'Hulbert Financial Digest',\n",
       "       \"Vanguard North Dakota's\", 'Enis Taner'], dtype='<U188')"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(np.unique(extract_company_list), 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
