{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following packages need to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install cleanco\n",
    "!pip install spacy\n",
    "!pip install whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from cleanco import cleanco\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "\n",
    "import re\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "import sys\n",
    "\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "from whoosh.index import open_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below is code for indexing the articles and store the indexed data in a folder named 'indexdir'. Thus, if the 'indexdir' folder exist, it will not reindex the dataset. To reindex, delete indexdir folder first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def createSearchableData(root):   \n",
    " \n",
    "    '''\n",
    "    Schema definition: title(name of file), path(as ID), content(indexed\n",
    "    but not stored),textdata (stored text content)\n",
    "    '''\n",
    "    schema = Schema(title=TEXT(stored=True),path=ID(stored=True),\\\n",
    "              content=TEXT,textdata=TEXT(stored=True))\n",
    "    if not os.path.exists(\"indexdir\"):\n",
    "        os.mkdir(\"indexdir\")\n",
    " \n",
    "        # Creating a index writer to add document as per schema\n",
    "        ix = create_in(\"indexdir\",schema)\n",
    "        writer = ix.writer()\n",
    "\n",
    "        filepaths = [os.path.join(root,i) for i in os.listdir(root)]\n",
    "        for path in filepaths:\n",
    "            fp = open(path,'r',encoding = \"utf-8\", errors='ignore')\n",
    "    #         print(path)\n",
    "            text = fp.read()\n",
    "            text = text.replace('â€”', '')\n",
    "            text = text.replace('(', '')\n",
    "            text = text.replace(')', '')\n",
    "            writer.add_document(title=path.split(\"\\\\\")[1], path=path,\\\n",
    "              content=text,textdata=text)\n",
    "            fp.close()\n",
    "        writer.commit()\n",
    " \n",
    "root = \"BI-articles\"\n",
    "createSearchableData(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(X, Y):\n",
    "    # tokenization \n",
    "    X_list = word_tokenize(X)  \n",
    "    Y_list = word_tokenize(Y) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # remove stop words from the string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_type(question):\n",
    "    template_1 = 'Which companies went bankrupt in month X of year Y'\n",
    "    template_2 = 'What affects GDP'\n",
    "    template_3 = 'What percentage of drop or increase is associated with Z'\n",
    "    template_4 = 'Who is the CEO of company X'\n",
    "    \n",
    "    template_list = [template_1, template_2, template_3, template_4]\n",
    "    \n",
    "    max_cosine_similarity = 0\n",
    "    question_type = -1\n",
    "    for i in range(len(template_list)):\n",
    "        template = template_list[i]\n",
    "        cosine_similarity = calc_cosine_similarity(question, template)\n",
    "#         print(cosine_similarity)\n",
    "        if cosine_similarity > max_cosine_similarity:\n",
    "            max_cosine_similarity = cosine_similarity\n",
    "            question_type = i\n",
    "    return question_type     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(question):\n",
    "    question_type = get_question_type(question)   \n",
    "    if question_type == -1:\n",
    "        print('Sorry we could not answer this question')\n",
    "    else:\n",
    "#         print(question_type)\n",
    "        if question_type == 0:\n",
    "            answer = bankrupt_answer(question)\n",
    "        elif question_type == 1:\n",
    "            answer = gdp_factors(question)\n",
    "        elif question_type == 2:\n",
    "            answer = gdp_factor_change(question)\n",
    "        elif question_type == 3:\n",
    "            answer = get_company_ceo(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bankrupt_answer(question):\n",
    "    nlp_result = nlp(question)\n",
    "    ents = nlp_result.ents\n",
    "    month = ''\n",
    "    year = ''\n",
    "    for ent in ents:\n",
    "        # 391 label are dates entities\n",
    "        if ent.label == 391:\n",
    "            for tag in pos_tag(word_tokenize(ent.text)):\n",
    "                if tag[1] == 'NNP':\n",
    "                    month = tag[0]\n",
    "                elif tag[1] == 'CD':\n",
    "                    year = tag[0]\n",
    "    if month == '' or year == '':\n",
    "        print(f'Please specify both month and year.')\n",
    "        return\n",
    "    \n",
    "    company_dict = get_bankrupt_from_date(year, month)\n",
    "    if company_dict == {}:\n",
    "        print(f'No company is found to declare banruptcy in {month} of {year}.')\n",
    "    elif len(company_dict.keys()) == 1:\n",
    "        print(f'{len(company_dict.keys())} company is found to declare banruptcy in {month} of {year}.')\n",
    "    else:\n",
    "        print(f'{len(company_dict.keys())} companies are found to declare banruptcy in {month} of {year}.')\n",
    "        \n",
    "    for key, value in company_dict.items():\n",
    "        articles = ' '.join(value)\n",
    "        print(f'{key}\\'s bankruptcy is supported by articles from {articles}.')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdp_factors(question):\n",
    "    GDP_factor_list = ['consumption', 'government spending', 'investment', 'exports']\n",
    "    answer = 'GDP is affected by ' + ', '.join(GDP_factor_list) + '.'\n",
    "    print(answer)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_percent_re(article_str):\n",
    "    percent_signs = re.findall('[0-9+-.,/]+%', article_str)\n",
    "    percent_texts = re.findall('[0-9+-.,/a-zA-Z]* ?percent+[a-zA-Z]*', article_str)\n",
    "    percent_point_texts = re.findall('[0-9+-.,/a-zA-Z]*\\s?percent+[a-zA-Z]*\\s?(?:point|Point)s?', article_str)\n",
    "    of_a_percent_point_texts = re.findall('[0-9+-.,/a-zA-Z]*\\sof\\sa\\s?percent+[a-zA-Z]*\\s?(?:point|Point)s?', article_str)\n",
    "    return percent_signs + percent_texts + percent_point_texts + of_a_percent_point_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdp_factor_change(question):\n",
    "    GDP_factor_list = ['consumption', 'government spending', 'investment', 'exports']\n",
    "    direction = 'factor not found'\n",
    "    for GDP_factor in GDP_factor_list:\n",
    "        if GDP_factor in question:\n",
    "            percent, direction, found_date = get_gdp_from_factor(GDP_factor)\n",
    "            input_factor = GDP_factor\n",
    "            if percent != -1:\n",
    "                if 'percentage' in percent.split(' ')[-1]:\n",
    "                    percent = percent + ' points'\n",
    "            break\n",
    "    if direction == 'factor not found':\n",
    "        print('The factor you asked is not identified as a factor that affects GDP')\n",
    "    elif direction == 'not found':\n",
    "        print('The current database does not have information about this factor\\'s impact on GDP')\n",
    "    elif direction == 'up':\n",
    "        print(f'{input_factor.capitalize()} increased GDP by {percent} according to a {found_date} article.')\n",
    "    elif direction == 'down':\n",
    "        print(f'{input_factor.capitalize()} decreased GDP by {percent} according to a {found_date} article.')\n",
    "    return\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_ceo(question):\n",
    "    nlp_result = nlp(question)\n",
    "    ents = nlp_result.ents\n",
    "    company_found = False\n",
    "    for ent in ents:\n",
    "        # 391 label are dates entities\n",
    "        if ent.label_ == \"ORG\":\n",
    "            company = ent.text\n",
    "            company_found = True\n",
    "            \n",
    "    if not company_found:\n",
    "        company_name_list = []\n",
    "        for token_pos in pos_tag(word_tokenize(question))[1:]:\n",
    "            if (token_pos[0].istitle() and (token_pos[1] == 'NNP')):\n",
    "                company_name_list.append(token_pos[0])\n",
    "        if len(company_name_list) == 0:        \n",
    "            print('The company name asked is invalid or not specified, please specify the company name.')\n",
    "            return\n",
    "        else:\n",
    "            company = ' '.join(company_name_list)\n",
    "    ceo, found_date = get_ceo_from_company(company)\n",
    "    if ceo == 'not found':\n",
    "        print(f'The CEO of {company} is not found')\n",
    "    else:\n",
    "        print(f'The CEO of {company} is {ceo} according to a {found_date} article')\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_company_name(company):\n",
    "    clean_company = cleanco(company).clean_name()\n",
    "    return clean_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bankrupt_from_date(year, month):\n",
    "    \n",
    "    \n",
    "    all_month_list = ['January', 'February', 'March', 'April' , 'May', 'June', 'July' , 'August', 'September', 'October', 'November', 'December']\n",
    "    all_year_list = [str(i) for i in range(1900,2014)]\n",
    "    \n",
    "    \n",
    "    ix = open_dir(\"indexdir\")\n",
    "    key_word_list = ['bankrupt', 'bankruptcy', 'Chapter', 'liquidation', 'liquidate']\n",
    "    # query_str is query string\n",
    "    query_str = ' OR '.join(key_word_list)\n",
    "    # Top 'n' documents as result\n",
    "    topN = 460\n",
    "    candidate_entities = {}\n",
    "    recent_day_list = ['today', 'yesterday', 'this Monday', 'this Tuesday', 'this Wednesday', 'this Thursday', 'this Friday', 'this week' 'last week']\n",
    "\n",
    "    key_phrase_list = ['went bankrupt', 'filed for bankruptcy', 'declared Chapter', 'liquidated', 'went liquidation']\n",
    "\n",
    "    with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "        query = QueryParser(\"content\", ix.schema).parse(query_str)\n",
    "        results = searcher.search(query,limit=topN)\n",
    "#             print(results)\n",
    "        for i in range(topN):\n",
    "    #         print(results[i]['title'], str(results[i].score))\n",
    "            raw_text = results[i]['textdata']\n",
    "            raw_text = raw_text.replace('Mt. Gox', 'Mt.Gox')\n",
    "            sentence_list = nltk.sent_tokenize(raw_text) \n",
    "            for j in range(len(sentence_list)):\n",
    "                sentence = sentence_list[j]\n",
    "                sentence = sentence.replace('Mt.Gox', 'Mt. Gox')\n",
    "#                     key_word_match = False\n",
    "#                     for key_word in key_word_list:\n",
    "#                         if key_word in sentence:\n",
    "#                             print('ddddddddddddddd', sentence)\n",
    "                recent_date_match = False       \n",
    "                for key_phrase in key_phrase_list:\n",
    "                    if key_phrase in sentence:\n",
    "                        key_phrase_pos = sentence.find(key_phrase)\n",
    "\n",
    "                        file_date = datetime.datetime.strptime(results[i]['title'][:10],'%Y-%m-%d')\n",
    "                        file_year = file_date.strftime('%Y')\n",
    "                        file_month = file_date.strftime('%B')\n",
    "\n",
    "\n",
    "                        have_specific_month = False\n",
    "                        for temp_month in all_month_list:\n",
    "                            if temp_month in sentence:\n",
    "                                have_specific_month = True\n",
    "\n",
    "                        have_specific_year = False\n",
    "                        for temp_year in all_year_list:\n",
    "                            if temp_year in sentence:\n",
    "                                have_specific_year = True\n",
    "\n",
    "                        if (month in sentence) and ((year in sentence) or ((year == file_year) and not have_specific_year)):\n",
    "\n",
    "\n",
    "#                             print('Time Match', sentence )\n",
    "                            nlp_result = nlp(sentence)\n",
    "                            ents = nlp_result.ents\n",
    "                            for ent in ents:\n",
    "                                if ent.label_ == 'ORG' or ent.label_ == 'PERSON':\n",
    "                                    if (ent.end_char == key_phrase_pos-1):\n",
    "#                                         print(sentence)\n",
    "                                        found_date = results[i]['title'][:10]\n",
    "                                        cleaned_company_name = clean_company_name(ent.text)\n",
    "                                        if cleaned_company_name not in candidate_entities.keys():\n",
    "                                            candidate_entities[cleaned_company_name] = [found_date]\n",
    "                                        else:\n",
    "                                            candidate_entities[cleaned_company_name].append(found_date)\n",
    "\n",
    "\n",
    "\n",
    "                        if not have_specific_month and not have_specific_year:\n",
    "                            if (file_year == year) and (file_month == month):\n",
    "#                                 print(file_year, year, file_month, month)\n",
    "#                                 print('File Match', sentence)\n",
    "#                                 for recent_day in recent_day_list:\n",
    "#                                     if recent_day in sentence:\n",
    "#                                         recent_date_match = True\n",
    "#                                 if recent_date_match:\n",
    "                                nlp_result = nlp(sentence)\n",
    "                                ents = nlp_result.ents\n",
    "\n",
    "                                phrase_found = False\n",
    "                                for ent in ents:\n",
    "                                    if ent.label_ == 'ORG' or ent.text=='Mt. Gox':\n",
    "                                        if (ent.end_char == key_phrase_pos-1):\n",
    "                                            phrase_found = True \n",
    "                                            found_date = results[i]['title'][:10]\n",
    "                                            cleaned_company_name = clean_company_name(ent.text)\n",
    "                                            if cleaned_company_name not in candidate_entities.keys():\n",
    "                                                candidate_entities[cleaned_company_name] = [found_date]\n",
    "                                            else:\n",
    "                                                candidate_entities[cleaned_company_name].append(found_date)\n",
    "\n",
    "#                                     if sentence[(key_phrase_pos-1-len('company')):(key_phrase_pos-1)] == 'company':\n",
    "#                                         print('COMPANY', sentence_list[j-1])\n",
    "#                                     if sentence[(key_phrase_pos-1-len('and')):(key_phrase_pos-1)] == 'and':\n",
    "#                                         print('AND', sentence_list[j])\n",
    "\n",
    "\n",
    "#         remove_str_list = ['cftc', 'reuters']\n",
    "#         for remove_str in remove_str_list: \n",
    "#             if remove_str in candidate_entities:\n",
    "#                 candidate_entities.remove(remove_str)\n",
    "    return candidate_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# all_month_list = ['January', 'February', 'March', 'April' , 'May', 'June', 'July' , 'August', 'September', 'October', 'November', 'December']\n",
    "# all_year_list = [str(i) for i in range(1900,2014)]\n",
    "# month_list = ['October']\n",
    "# year_list = ['2013']\n",
    "# result_dict = {}\n",
    "# for year in year_list:\n",
    "#     for month in month_list:\n",
    "#         ix = open_dir(\"indexdir\")\n",
    "#         key_word_list = ['bankrupt', 'bankruptcy', 'Chapter', 'liquidation', 'liquidate']\n",
    "#         # query_str is query string\n",
    "#         query_str = ' OR '.join(key_word_list)\n",
    "#         # Top 'n' documents as result\n",
    "#         topN = 460\n",
    "#         candidate_entities = {}\n",
    "#         recent_day_list = ['today', 'yesterday', 'this Monday', 'this Tuesday', 'this Wednesday', 'this Thursday', 'this Friday', 'this week' 'last week']\n",
    "        \n",
    "#         key_phrase_list = ['went bankrupt', 'filed for bankruptcy', 'declared Chapter', 'liquidated', 'went liquidation']\n",
    "        \n",
    "#         with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "#             query = QueryParser(\"content\", ix.schema).parse(query_str)\n",
    "#             results = searcher.search(query,limit=topN)\n",
    "# #             print(results)\n",
    "#             for i in range(topN):\n",
    "#         #         print(results[i]['title'], str(results[i].score))\n",
    "#                 raw_text = results[i]['textdata']\n",
    "#                 raw_text = raw_text.replace('Mt. Gox', 'Mt.Gox')\n",
    "#                 sentence_list = nltk.sent_tokenize(raw_text) \n",
    "#                 for j in range(len(sentence_list)):\n",
    "#                     sentence = sentence_list[j]\n",
    "#                     sentence = sentence.replace('Mt.Gox', 'Mt. Gox')\n",
    "# #                     key_word_match = False\n",
    "# #                     for key_word in key_word_list:\n",
    "# #                         if key_word in sentence:\n",
    "# #                             print('ddddddddddddddd', sentence)\n",
    "#                     recent_date_match = False       \n",
    "#                     for key_phrase in key_phrase_list:\n",
    "#                         if key_phrase in sentence:\n",
    "#                             key_phrase_pos = sentence.find(key_phrase)\n",
    "                            \n",
    "#                             file_date = datetime.datetime.strptime(results[i]['title'][:10],'%Y-%m-%d')\n",
    "#                             file_year = file_date.strftime('%Y')\n",
    "#                             file_month = file_date.strftime('%B')\n",
    "                            \n",
    "                                            \n",
    "#                             have_specific_month = False\n",
    "#                             for temp_month in all_month_list:\n",
    "#                                 if temp_month in sentence:\n",
    "#                                     have_specific_month = True\n",
    "\n",
    "#                             have_specific_year = False\n",
    "#                             for temp_year in all_year_list:\n",
    "#                                 if temp_year in sentence:\n",
    "#                                     have_specific_year = True\n",
    "        \n",
    "#                             if (month in sentence) and ((year in sentence) or ((year == file_year) and not have_specific_year)):\n",
    "                \n",
    "                \n",
    "#                                 print('Time Match', sentence )\n",
    "#                                 nlp_result = nlp(sentence)\n",
    "#                                 ents = nlp_result.ents\n",
    "#                                 for ent in ents:\n",
    "#                                     if ent.label_ == 'ORG' or ent.label_ == 'PERSON':\n",
    "#                                         if (ent.end_char == key_phrase_pos-1):\n",
    "#                                             print(sentence)\n",
    "#                                             found_date = results[i]['title'][:10]\n",
    "#                                             cleaned_company_name = clean_company_name(ent.text)\n",
    "#                                             if cleaned_company_name not in candidate_entities.keys():\n",
    "#                                                 candidate_entities[cleaned_company_name] = [found_date]\n",
    "#                                             else:\n",
    "#                                                 candidate_entities[cleaned_company_name].append(found_date)\n",
    "                            \n",
    "\n",
    "                                    \n",
    "#                             if not have_specific_month and not have_specific_year:\n",
    "#                                 if (file_year == year) and (file_month == month):\n",
    "#                                     print(file_year, year, file_month, month)\n",
    "#                                     print('File Match', sentence)\n",
    "#     #                                 for recent_day in recent_day_list:\n",
    "#     #                                     if recent_day in sentence:\n",
    "#     #                                         recent_date_match = True\n",
    "#     #                                 if recent_date_match:\n",
    "#                                     nlp_result = nlp(sentence)\n",
    "#                                     ents = nlp_result.ents\n",
    "                                    \n",
    "#                                     phrase_found = False\n",
    "#                                     for ent in ents:\n",
    "#                                         if ent.label_ == 'ORG' or ent.text=='Mt. Gox':\n",
    "#                                             if (ent.end_char == key_phrase_pos-1):\n",
    "#                                                 phrase_found = True \n",
    "#                                                 found_date = results[i]['title'][:10]\n",
    "#                                                 cleaned_company_name = clean_company_name(ent.text)\n",
    "#                                                 if cleaned_company_name not in candidate_entities.keys():\n",
    "#                                                     candidate_entities[cleaned_company_name] = [found_date]\n",
    "#                                                 else:\n",
    "#                                                     candidate_entities[cleaned_company_name].append(found_date)\n",
    "                                                    \n",
    "# #                                     if sentence[(key_phrase_pos-1-len('company')):(key_phrase_pos-1)] == 'company':\n",
    "# #                                         print('COMPANY', sentence_list[j-1])\n",
    "# #                                     if sentence[(key_phrase_pos-1-len('and')):(key_phrase_pos-1)] == 'and':\n",
    "# #                                         print('AND', sentence_list[j])\n",
    "\n",
    "                        \n",
    "# #         remove_str_list = ['cftc', 'reuters']\n",
    "# #         for remove_str in remove_str_list: \n",
    "# #             if remove_str in candidate_entities:\n",
    "# #                 candidate_entities.remove(remove_str)\n",
    "#         print(candidate_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdp_from_factor(factor):\n",
    "    look_length_left = 80\n",
    "\n",
    "    key_word = 'gdp'\n",
    "    \n",
    "    closest_distance = look_length_left\n",
    "\n",
    "\n",
    "    excluded_word_list = ['up to', 'down to']\n",
    "    up_word_list = ['up', 'increase', 'rise', 'add']\n",
    "    down_word_list = ['down', 'decrease', 'drop']\n",
    "    change_word_list = up_word_list + down_word_list\n",
    "\n",
    "\n",
    "    ix = open_dir(\"indexdir\")\n",
    "    query_str = factor + ' AND ' + '(' + 'GDP' +')'\n",
    "    topN = 100\n",
    "    result = []\n",
    "    with ix.searcher(weighting=scoring.TF_IDF()) as searcher2:\n",
    "        query = QueryParser(\"content\", ix.schema).parse(query_str)\n",
    "        results = searcher2.search(query,limit=topN)\n",
    "#         print(results)\n",
    "        found = False\n",
    "        direction = ''\n",
    "        found_date = ''\n",
    "        for i in range(topN):\n",
    "#             print(results[i]['title'], str(results[i].score))\n",
    "            raw_text = results[i]['textdata'].lower()\n",
    "            sentence_list = nltk.sent_tokenize(raw_text)  \n",
    "            for sentence in sentence_list:\n",
    "                sentence_ignore = False\n",
    "                for excluded_word in excluded_word_list:\n",
    "                    if excluded_word in sentence:\n",
    "                        sentence_ignore = True\n",
    "    #             sentence_token_list = word_tokenize(sentence)\n",
    "    #             sentence_token_tag_list = nltk.pos_tag(sentence_token_list)\n",
    "                \n",
    "                if not sentence_ignore:\n",
    "                    key_word_match = False\n",
    "                    if key_word.lower() in sentence:\n",
    "                        key_word_match = True\n",
    "                    \n",
    "                    if key_word_match and (factor in sentence):\n",
    "#                         print(sentence)\n",
    "                        for up_word in up_word_list:\n",
    "                            if up_word in sentence:\n",
    "                                key_word_pos_list = [i.start() for i in re.finditer(key_word, sentence)]\n",
    "                                for key_word_pos in key_word_pos_list:\n",
    "                                    if key_word_pos >=3:\n",
    "                                        if sentence[key_word_pos-3:key_word_pos-1] == 'of':\n",
    "                                            break\n",
    "                                    word_pos = sentence.find(up_word)\n",
    "                                    start = max(0,word_pos-look_length_left)\n",
    "                                    end = min(len(sentence)+1,word_pos+look_length_left+6)\n",
    "#                                     print(start)\n",
    "#                                     print(end)\n",
    "                                    sentence_snippet = sentence[start:end]\n",
    "                                    key_word_match = False\n",
    "                                    if key_word.lower() in sentence_snippet:\n",
    "                                        key_word_match = True\n",
    "                                    if (key_word_pos > word_pos) and key_word_match and factor in sentence_snippet:      \n",
    "                                        all_percent_list = extract_all_percent_re(sentence[word_pos:end])\n",
    "                                        if len(all_percent_list) > 0:\n",
    "        #                                     if (all_percent_list) == 1:\n",
    "        #                                         closest_percent = all_percent_list[0]\n",
    "        #                                     else:\n",
    "                                            for percent in all_percent_list:\n",
    "                                                percent_start = sentence.find(percent)\n",
    "                                                if percent_start > word_pos:\n",
    "                                                    distance = abs(percent_start+len(percent)-key_word_pos)\n",
    "        #                                             else:\n",
    "        #                                                 distance = abs(percent_start-word_pos)\n",
    "                                                    if distance < closest_distance:\n",
    "                                                        closest_percent = percent\n",
    "                                                        closest_distance = distance\n",
    "                                                        found = True\n",
    "#                                                         print(sentence_snippet)\n",
    "                                                        direction = 'up'\n",
    "                                                        found_date = results[i]['title'][:10]\n",
    "#                                             print(sentence_snippet)\n",
    "#                                             print(all_percent_list)\n",
    "        #                                     result = closest_percent\n",
    "    #                     if found:\n",
    "    #                         break\n",
    "\n",
    "                        for down_word in down_word_list:\n",
    "                            if down_word in sentence:\n",
    "                                key_word_pos_list = [i.start() for i in re.finditer(key_word, sentence)]\n",
    "                                for key_word_pos in key_word_pos_list:\n",
    "                                    if key_word_pos >=3:\n",
    "                                        if sentence[key_word_pos-3:key_word_pos-1] == 'of':\n",
    "                                            break\n",
    "                                    word_pos = sentence.find(down_word)\n",
    "                                    start = max(0,word_pos-look_length_left)\n",
    "                                    end = min(len(sentence)+1,word_pos+look_length_left+6)\n",
    "#                                     print(start)\n",
    "#                                     print(end)\n",
    "                                    sentence_snippet = sentence[start:end]\n",
    "                                    key_word_match = False\n",
    "                                    if key_word.lower() in sentence_snippet:\n",
    "                                        key_word_match = True\n",
    "                                    if (key_word_pos > word_pos) and key_word_match and factor in sentence_snippet:      \n",
    "                                        all_percent_list = extract_all_percent_re(sentence[word_pos:end])\n",
    "                                        if len(all_percent_list) > 0:\n",
    "        #                                     if (all_percent_list) == 1:\n",
    "        #                                         closest_percent = all_percent_list[0]\n",
    "        #                                     else:\n",
    "                                            for percent in all_percent_list:\n",
    "                                                percent_start = sentence.find(percent)\n",
    "                                                if percent_start > word_pos:\n",
    "                                                    distance = abs(percent_start+len(percent)-key_word_pos)\n",
    "        #                                             else:\n",
    "        #                                                 distance = abs(percent_start-word_pos)\n",
    "                                                    if distance < closest_distance:\n",
    "                                                        closest_percent = percent\n",
    "                                                        closest_distance = distance\n",
    "                                                        found = True\n",
    "#                                                         print(sentence_snippet)\n",
    "                                                        direction = 'down'\n",
    "                                                        date = ''\n",
    "                                                        found_date = results[i]['title'][:10]\n",
    "#                                         print(sentence)\n",
    "#                                         print(sentence_snippet)\n",
    "#                                         print(all_percent_list)\n",
    "    #                                     result = closest_percent\n",
    "    #                                     break\n",
    "#                     if found:\n",
    "#                         break\n",
    "#                 if found:\n",
    "#                     break\n",
    "#             if found:\n",
    "#                 break\n",
    "        if found:\n",
    "            return closest_percent, direction, found_date\n",
    "        else:\n",
    "            return -1, 'not found', found_date\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ceo_from_company(company):\n",
    "#     look_length_left = 80\n",
    "    key_word_list = ['CEO', 'chief executive officer']\n",
    "    \n",
    "    \n",
    "    closest_distance = 500\n",
    "\n",
    "\n",
    "#     excluded_word_list = ['up to', 'down to']\n",
    "#     up_word_list = ['up', 'increase', 'rise', 'add']\n",
    "#     down_word_list = ['down', 'decrease', 'drop']\n",
    "#     change_word_list = up_word_list + down_word_list\n",
    "\n",
    "\n",
    "    ix = open_dir(\"indexdir\")\n",
    "    query_str = company + ' AND ' + '(' + 'GDP' +')'\n",
    "    topN = 100\n",
    "    result = []\n",
    "    with ix.searcher(weighting=scoring.TF_IDF()) as searcher2:\n",
    "        query = QueryParser(\"content\", ix.schema).parse(query_str)\n",
    "        results = searcher2.search(query,limit=topN)\n",
    "#         print(results)\n",
    "        found = False\n",
    "        for i in range(topN):\n",
    "            try: \n",
    "                results[i]\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "#             print(results[i]['title'], str(results[i].score))\n",
    "            raw_text = results[i]['textdata']\n",
    "            sentence_list = nltk.sent_tokenize(raw_text)  \n",
    "            for sentence in sentence_list:\n",
    "                sentence_ignore = False\n",
    "#                 for excluded_word in excluded_word_list:\n",
    "#                     if excluded_word in sentence:\n",
    "#                         sentence_ignore = True\n",
    "    #             sentence_token_list = word_tokenize(sentence)\n",
    "    #             sentence_token_tag_list = nltk.pos_tag(sentence_token_list)\n",
    "                if not sentence_ignore:\n",
    "                    sentence = sentence.replace('REUTERS/Kim White', '')\n",
    "                    key_word_match = False\n",
    "                    for key_word in key_word_list:\n",
    "                        if key_word.lower() in sentence.lower():\n",
    "                            key_word_match = True\n",
    "                    if key_word_match and (company in sentence):\n",
    "                        for key_word in key_word_list:\n",
    "#                             print(sentence)\n",
    "                            key_word_pos_list = [i.start() for i in re.finditer(key_word, sentence)]\n",
    "                            for key_word_pos in key_word_pos_list:\n",
    "    #                             if key_word_pos >=3:\n",
    "    #                                 if sentence[key_word_pos-3:key_word_pos-1] == 'of':\n",
    "    #                                     break\n",
    "                                company_pos = sentence.find(company)\n",
    "                                \n",
    "                                ents = nlp(sentence).ents\n",
    "                                for ent in ents:\n",
    "                                    person_name_list = ent.text.split(' ')\n",
    "                                    if len(person_name_list) >= 2:\n",
    "                                        first_name = ent.text.split(' ')[0]\n",
    "                                        last_name = ent.text.split(' ')[1]\n",
    "                                        if (len(re.findall(r'[A-Z]',first_name)) == 1) and (len(re.findall(r'[A-Z]',last_name)) == 1):\n",
    "                                            if (ent.label_ == 'PERSON') and ent.text:\n",
    "\n",
    "                                                if ent.text.lower()[-2:] == '\\'s': \n",
    "                                                    person_name = ent.text[:-2]\n",
    "        #                                             name_has_possesive_at_end.append(True)\n",
    "                                                else:\n",
    "                                                    person_name = ent.text\n",
    "        #                                             name_has_possesive_at_end.append(False)\n",
    "\n",
    "                                                person_pos = ent.start_char\n",
    "\n",
    "                                                if person_pos < key_word_pos:\n",
    "                                                    distance = abs(person_pos+len(ent.text)-key_word_pos)\n",
    "                                                else:\n",
    "                                                    distance = abs(person_pos-(key_word_pos+len(key_word)))\n",
    "                                                if distance < closest_distance:\n",
    "                                                    closest_distance = distance\n",
    "                                                    closest_person = ent.text\n",
    "                                                    found = True\n",
    "                                                    found_date = results[i]['title'][:10]\n",
    "        if found:\n",
    "            return closest_person, found_date\n",
    "        else:\n",
    "            return 'not found', 'not found'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Example Q&A Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 companies are found to declare banruptcy in October of 2013.\n",
      "Dias's bankruptcy is supported by articles from 2013-12-16.\n",
      "OGX's bankruptcy is supported by articles from 2013-12-25.\n"
     ]
    }
   ],
   "source": [
    "main('Which companies went bankrupt in month October of year 2013?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 companies are found to declare banruptcy in September of 2008.\n",
      "Lehman Brothers's bankruptcy is supported by articles from 2013-09-13 2014-02-21.\n",
      "Lehman Brothers Holdings's bankruptcy is supported by articles from 2013-01-22 2014-03-26.\n"
     ]
    }
   ],
   "source": [
    "main('Which companies went bankrupt in month September of year 2008?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 company is found to declare banruptcy in March of 2014.\n",
      "Mt. Gox's bankruptcy is supported by articles from 2014-03-29 2014-03-13.\n"
     ]
    }
   ],
   "source": [
    "main('Which companies went bankrupt in month March of year 2014?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No company is found to declare banruptcy in June of 2014.\n"
     ]
    }
   ],
   "source": [
    "main('Which companies went bankrupt in month June of year 2014?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDP is affected by consumption, government spending, investment, exports.\n"
     ]
    }
   ],
   "source": [
    "main('What affects GDP?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumption increased GDP by 2 percentage points according to a 2014-04-30 article.\n"
     ]
    }
   ],
   "source": [
    "main('What percentage of drop or increase is associated with consumption?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Government spending increased GDP by 0.83 percentage points according to a 2014-10-30 article.\n"
     ]
    }
   ],
   "source": [
    "main('What percentage of drop or increase is associated with government spending?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment increased GDP by 1.18 percentage points according to a 2013-01-30 article.\n"
     ]
    }
   ],
   "source": [
    "main('What percentage of drop or increase is associated with investment?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exports decreased GDP by 10% according to a 2014-08-06 article.\n"
     ]
    }
   ],
   "source": [
    "main('What percentage of drop or increase is associated with exports?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CEO of Leucadia is Dick Handler according to a 2013-09-18 article\n"
     ]
    }
   ],
   "source": [
    "main('Who is the CEO of Leucadia?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CEO of General Electric is Jeffrey Immelt according to a 2014-10-17 article\n"
     ]
    }
   ],
   "source": [
    "main('Who is the CEO of General Electric?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CEO of Twitter is Dick Costolo according to a 2013-11-07 article\n"
     ]
    }
   ],
   "source": [
    "main('Who is the CEO of Twitter?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New experiments could be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
